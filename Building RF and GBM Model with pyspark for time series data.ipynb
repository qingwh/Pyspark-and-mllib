{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Import SparkSession from pyspark.sql\n",
    "from pyspark.sql import SparkSession\n",
    "# Create my_spark\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "# load ratings data\n",
    "df = spark.read.csv(\"2017_StPaul_MN_Real_Estate.csv\", header=True, inferSchema=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "df=df.withColumn('offmarketdate', F.unix_timestamp(\"offmarketdate\", \"MM/dd/yyyy H:mm\").cast(\"timestamp\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df=df.withColumn('LISTDATE', F.unix_timestamp(\"LISTDATE\", \"MM/dd/yyyy H:mm\").cast(\"timestamp\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+\n",
      "|      offmarketdate|\n",
      "+-------------------+\n",
      "|0017-07-30 00:00:00|\n",
      "|0017-10-13 00:00:00|\n",
      "|0017-07-24 00:00:00|\n",
      "|0017-09-13 00:00:00|\n",
      "|0017-10-03 00:00:00|\n",
      "|0017-04-27 00:00:00|\n",
      "|0017-07-10 00:00:00|\n",
      "|0017-11-10 00:00:00|\n",
      "|0017-11-11 00:00:00|\n",
      "|0017-11-20 00:00:00|\n",
      "|0017-08-07 00:00:00|\n",
      "|0017-07-29 00:00:00|\n",
      "|0017-03-24 00:00:00|\n",
      "|0017-06-11 00:00:00|\n",
      "|0017-06-05 00:00:00|\n",
      "|0017-10-17 00:00:00|\n",
      "|0017-09-22 00:00:00|\n",
      "|0017-10-16 00:00:00|\n",
      "|0017-06-06 00:00:00|\n",
      "|0017-08-27 00:00:00|\n",
      "+-------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df[['offmarketdate']].show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+\n",
      "|           LISTDATE|\n",
      "+-------------------+\n",
      "|0017-07-15 00:00:00|\n",
      "|0017-10-09 00:00:00|\n",
      "|0017-06-26 00:00:00|\n",
      "|0017-08-25 00:00:00|\n",
      "|0017-09-12 00:00:00|\n",
      "|0017-04-10 00:00:00|\n",
      "|0017-06-08 00:00:00|\n",
      "|0017-11-05 00:00:00|\n",
      "|0017-10-12 00:00:00|\n",
      "|0017-09-02 00:00:00|\n",
      "|0017-05-19 00:00:00|\n",
      "|0017-05-11 00:00:00|\n",
      "|0017-03-12 00:00:00|\n",
      "|0017-03-06 00:00:00|\n",
      "|0017-05-18 00:00:00|\n",
      "|0017-10-15 00:00:00|\n",
      "|0017-09-10 00:00:00|\n",
      "|0017-09-01 00:00:00|\n",
      "|0017-05-06 00:00:00|\n",
      "|0017-08-11 00:00:00|\n",
      "+-------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df[['LISTDATE']].show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df=df.select('SalesClosePrice','LISTDATE','LISTPRICE','OriginalListPrice','SchoolDistrictNumber','offmarketdate','LivingArea','FOUNDATIONSIZE', 'DAYSONMARKET', 'Fireplaces', 'PDOM', 'SQFTABOVEGROUND', 'YEARBUILT', 'ACRES', 'BathsFull', 'BathsHalf', 'BATHQUARTER', 'BATHSTHREEQUARTER', 'BATHSTOTAL', 'SQFTBELOWGROUND', 'AssociationFee', 'AssessedValuation')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- SalesClosePrice: integer (nullable = true)\n",
      " |-- LISTDATE: timestamp (nullable = true)\n",
      " |-- LISTPRICE: integer (nullable = true)\n",
      " |-- OriginalListPrice: integer (nullable = true)\n",
      " |-- SchoolDistrictNumber: string (nullable = true)\n",
      " |-- offmarketdate: timestamp (nullable = true)\n",
      " |-- LivingArea: integer (nullable = true)\n",
      " |-- FOUNDATIONSIZE: integer (nullable = true)\n",
      " |-- DAYSONMARKET: integer (nullable = true)\n",
      " |-- Fireplaces: integer (nullable = true)\n",
      " |-- PDOM: integer (nullable = true)\n",
      " |-- SQFTABOVEGROUND: integer (nullable = true)\n",
      " |-- YEARBUILT: integer (nullable = true)\n",
      " |-- ACRES: double (nullable = true)\n",
      " |-- BathsFull: integer (nullable = true)\n",
      " |-- BathsHalf: integer (nullable = true)\n",
      " |-- BATHQUARTER: integer (nullable = true)\n",
      " |-- BATHSTHREEQUARTER: integer (nullable = true)\n",
      " |-- BATHSTOTAL: integer (nullable = true)\n",
      " |-- SQFTBELOWGROUND: integer (nullable = true)\n",
      " |-- AssociationFee: integer (nullable = true)\n",
      " |-- AssessedValuation: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------------------+------------+-------------+\n",
      "|         SCHOOLDISTRICTNUMBER|School_Index|   School_Vec|\n",
      "+-----------------------------+------------+-------------+\n",
      "|             834 - Stillwater|         3.0|(7,[3],[1.0])|\n",
      "|             834 - Stillwater|         3.0|(7,[3],[1.0])|\n",
      "|622 - North St Paul-Maplewood|         1.0|(7,[1],[1.0])|\n",
      "|             834 - Stillwater|         3.0|(7,[3],[1.0])|\n",
      "|622 - North St Paul-Maplewood|         1.0|(7,[1],[1.0])|\n",
      "|             834 - Stillwater|         3.0|(7,[3],[1.0])|\n",
      "|             834 - Stillwater|         3.0|(7,[3],[1.0])|\n",
      "|             834 - Stillwater|         3.0|(7,[3],[1.0])|\n",
      "|             834 - Stillwater|         3.0|(7,[3],[1.0])|\n",
      "|             834 - Stillwater|         3.0|(7,[3],[1.0])|\n",
      "|             834 - Stillwater|         3.0|(7,[3],[1.0])|\n",
      "|             834 - Stillwater|         3.0|(7,[3],[1.0])|\n",
      "|             834 - Stillwater|         3.0|(7,[3],[1.0])|\n",
      "|             834 - Stillwater|         3.0|(7,[3],[1.0])|\n",
      "|             834 - Stillwater|         3.0|(7,[3],[1.0])|\n",
      "|             834 - Stillwater|         3.0|(7,[3],[1.0])|\n",
      "|             834 - Stillwater|         3.0|(7,[3],[1.0])|\n",
      "|             834 - Stillwater|         3.0|(7,[3],[1.0])|\n",
      "|             834 - Stillwater|         3.0|(7,[3],[1.0])|\n",
      "|             834 - Stillwater|         3.0|(7,[3],[1.0])|\n",
      "+-----------------------------+------------+-------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import OneHotEncoder, StringIndexer\n",
    "\n",
    "# Map strings to numbers with string indexer\n",
    "string_indexer = StringIndexer(inputCol='SchoolDistrictNumber', outputCol='School_Index')\n",
    "indexed_df = string_indexer.fit(df).transform(df)\n",
    "\n",
    "# Onehot encode indexed values\n",
    "encoder = OneHotEncoder(inputCol='School_Index', outputCol='School_Vec')\n",
    "encoded_df = encoder.transform(indexed_df)\n",
    "\n",
    "# Inspect the transformation steps\n",
    "encoded_df[['SCHOOLDISTRICTNUMBER', 'School_Index', 'School_Vec']].show(truncate=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+--------------------+-------------------+-------------------+\n",
      "|SalesClosePrice|            features|      offmarketdate|           LISTDATE|\n",
      "+---------------+--------------------+-------------------+-------------------+\n",
      "|         143000|(25,[0,1,5,9,10,1...|0017-07-30 00:00:00|0017-07-15 00:00:00|\n",
      "|         190000|(25,[0,1,5,9,10,1...|0017-10-13 00:00:00|0017-10-09 00:00:00|\n",
      "|         225000|(25,[0,1,3,9,10,1...|0017-07-24 00:00:00|0017-06-26 00:00:00|\n",
      "|         265000|(25,[0,1,5,9,10,1...|0017-09-13 00:00:00|0017-08-25 00:00:00|\n",
      "|         249900|(25,[0,1,3,9,10,1...|0017-10-03 00:00:00|0017-09-12 00:00:00|\n",
      "+---------------+--------------------+-------------------+-------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import VectorAssembler\n",
    "# Replace Missing values\n",
    "encoded_df = encoded_df.fillna(-1)\n",
    "features_cols=['LISTPRICE','OriginalListPrice','School_Vec','FOUNDATIONSIZE', 'DAYSONMARKET', 'Fireplaces', 'PDOM', 'SQFTABOVEGROUND', 'LivingArea', 'YEARBUILT', 'ACRES', 'BathsFull', 'BathsHalf', 'BATHQUARTER', 'BATHSTHREEQUARTER', 'BATHSTOTAL', 'SQFTBELOWGROUND', 'AssociationFee', 'AssessedValuation']\n",
    "# Create the vector assembler transformer\n",
    "vec = VectorAssembler(inputCols=features_cols, outputCol='features')\n",
    "# Apply the vector transformer to data\n",
    "encoded_df = vec.transform(encoded_df)\n",
    "# Select only the feature vectors and the dependent variable\n",
    "ml_ready_df = encoded_df.select(['SalesClosePrice','features','offmarketdate','LISTDATE'])\n",
    "# Inspect Results\n",
    "ml_ready_df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import datediff, to_date, lit\n",
    "\n",
    "split_date = to_date(lit('0017-09-01'))\n",
    "# Create Sequential Test set\n",
    "# Create Sequential Test and Training Sets\n",
    "train_df = ml_ready_df.where(ml_ready_df['offmarketdate'] < split_date) \n",
    "test_df = ml_ready_df.where(ml_ready_df['offmarketdate'] >= split_date ).where(ml_ready_df['LISTDATE'] <= split_date )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.regression import GBTRegressor\n",
    "\n",
    "# Train a Gradient Boosted Trees (GBT) model.\n",
    "gbt = GBTRegressor(featuresCol='features',\n",
    "                           labelCol='SalesClosePrice',\n",
    "                           predictionCol=\"Prediction_Price\",\n",
    "                           seed=42\n",
    "                           )\n",
    "\n",
    "# Train model.\n",
    "gbt_model = gbt.fit(train_df)\n",
    "\n",
    "gbt_predictions=gbt_model.transform(test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+--------------------+-------------------+-------------------+------------------+\n",
      "|SalesClosePrice|            features|      offmarketdate|           LISTDATE|  Prediction_Price|\n",
      "+---------------+--------------------+-------------------+-------------------+------------------+\n",
      "|         265000|(25,[0,1,5,9,10,1...|0017-09-13 00:00:00|0017-08-25 00:00:00| 233188.2078303583|\n",
      "|         274000|(25,[0,1,5,9,10,1...|0017-10-16 00:00:00|0017-09-01 00:00:00|285099.13697210734|\n",
      "|         289900|[299900.0,299900....|0017-09-30 00:00:00|0017-06-26 00:00:00| 302697.0272815767|\n",
      "|         375000|[383000.0,400000....|0017-09-18 00:00:00|0017-07-09 00:00:00| 366223.4141945253|\n",
      "|         394900|(25,[0,1,5,9,10,1...|0017-09-18 00:00:00|0017-07-14 00:00:00| 409191.4678542085|\n",
      "|         394900|[394900.0,424900....|0017-11-09 00:00:00|0017-06-16 00:00:00| 395128.1138736871|\n",
      "|         404005|[409990.0,436255....|0017-11-02 00:00:00|0017-06-12 00:00:00|424292.19977619435|\n",
      "|         405000|(25,[0,1,7,9,10,1...|0017-10-18 00:00:00|0017-07-17 00:00:00|425938.57358222746|\n",
      "|         452990|(25,[0,1,5,9,10,1...|0017-09-23 00:00:00|0017-04-23 00:00:00|474569.39835793775|\n",
      "|         475000|(25,[0,1,7,9,10,1...|0017-09-15 00:00:00|0017-07-20 00:00:00| 462917.3534228494|\n",
      "|         468000|(25,[0,1,5,9,10,1...|0017-11-26 00:00:00|0017-06-05 00:00:00| 468438.9887135534|\n",
      "|         485000|[490000.0,490000....|0017-09-08 00:00:00|0017-08-08 00:00:00|462616.83414165024|\n",
      "|         484400|[499900.0,499900....|0017-10-11 00:00:00|0017-07-10 00:00:00|461441.37532149855|\n",
      "|         510000|[519900.0,559900....|0017-11-05 00:00:00|0017-05-23 00:00:00| 523892.9194644285|\n",
      "|         647000|[679900.0,679900....|0017-09-08 00:00:00|0017-07-14 00:00:00| 776106.9664110987|\n",
      "|         660000|[699000.0,699000....|0017-09-18 00:00:00|0017-07-14 00:00:00| 830190.4416924582|\n",
      "|         118500|(25,[0,1,3,9,10,1...|0017-09-08 00:00:00|0017-08-07 00:00:00|122390.72317042429|\n",
      "|         119500|(25,[0,1,3,9,10,1...|0017-09-15 00:00:00|0017-08-22 00:00:00|123733.10143383135|\n",
      "|         132000|(25,[0,1,3,9,10,1...|0017-09-21 00:00:00|0017-08-05 00:00:00|143989.63763671782|\n",
      "|         150000|(25,[0,1,3,9,10,1...|0017-09-18 00:00:00|0017-08-09 00:00:00|145486.18989286106|\n",
      "+---------------+--------------------+-------------------+-------------------+------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "gbt_predictions.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.regression import RandomForestRegressor\n",
    "# Initialize model with columns to utilize\n",
    "rf = RandomForestRegressor(featuresCol=\"features\",labelCol=\"SalesClosePrice\",predictionCol=\"Prediction_Price\",seed=42)\n",
    "\n",
    "# Train model\n",
    "rf_model = rf.fit(train_df)\n",
    "rf_predictions=rf_model.transform(test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Boosted Trees RMSE: 41242.35692384228\n",
      "Gradient Boosted Trees R^2: 0.9248955220962971\n",
      "Random Forest Regression RMSE: 39255.218653369346\n",
      "Random Forest Regression R^2: 0.9319585312092161\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "\n",
    "# Select columns to compute test error\n",
    "evaluator = RegressionEvaluator(labelCol='SalesClosePrice', \n",
    "                                predictionCol='Prediction_Price')\n",
    "# Dictionary of model predictions to loop over\n",
    "models = {'Gradient Boosted Trees': gbt_predictions, 'Random Forest Regression': rf_predictions}\n",
    "for key, preds in models.items():\n",
    "  # Create evaluation metrics\n",
    "  rmse = evaluator.evaluate(preds, {evaluator.metricName: 'rmse'})\n",
    "  r2 = evaluator.evaluate(preds, {evaluator.metricName: 'r2'})\n",
    "  \n",
    "  # Print Model Metrics\n",
    "  print(key + ' RMSE: ' + str(rmse))\n",
    "  print(key + ' R^2: ' + str(r2))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
